{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification for stop or behavior signals in front of Road Following model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model is the Controller followed by Road following model either from Jetbot or Jetracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch2trt import TRTModule\n",
    "import torchvision\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "#for classification with behavior per class/index -\n",
    "CATEGORIES_classification = ['background1','redlight','greenlight','bottle']\n",
    "model_trt_classification = TRTModule()\n",
    "model_trt_classification.load_state_dict(torch.load('trt_classification_behavior_model.pth.pth')) #category jetracer model for jetbot model from training and build with TRT\n",
    "\n",
    "\n",
    "# Road following script from Jetbot, if from Jetraceer with categories add ['apex','bottle']\n",
    "CATEGORIES = []\n",
    "category_roadindex= 0 # len(CATEGORIES) Categories can not be switched while driving \n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load('best_steering_model_xy_trt.pth')) #jetbot road following model from Jetbot road following training and build TRT (no category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pre-Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded our model, but there's a slight issue. The format that we trained our model doesnt exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\n",
    "\n",
    "1. Convert from HWC layout to CHW layout\n",
    "2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "3. Transfer the data from CPU memory to GPU memory\n",
    "4. Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<traitlets.traitlets.directional_link at 0x7ed4647b00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "import ipywidgets.widgets as widgets\n",
    "import time\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import traitlets\n",
    "\n",
    "target_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "x_slider = ipywidgets.FloatSlider(min=-1.5, max=1.5, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=-1.5, max=1.5, orientation='horizontal', description='y')\n",
    "\n",
    "def display_xy(camera_image):\n",
    "    image = np.copy(camera_image)\n",
    "    x = x_slider.value\n",
    "    y = y_slider.value\n",
    "    x = int(x * 224 / 2 + 112)\n",
    "    y = int(y * -224 / 2 + 112)\n",
    "    image = cv2.circle(image, (x, y), 8, (0, 255, 0), 3)\n",
    "    image = cv2.circle(image, (112, 224), 8, (0, 0,255), 3)\n",
    "    image = cv2.line(image, (x,y), (112,224), (255,0,0), 3)\n",
    "    jpeg_image = bgr8_to_jpeg(image)\n",
    "    return jpeg_image\n",
    "\n",
    "#time.sleep(1)\n",
    "traitlets.dlink((camera, 'value'), (target_widget, 'value'), transform=display_xy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create our robot instance which we'll need to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPS frames per second,\n",
    "* blocked: probability of blocked road (automatic updated), Manu. time...: how many frames bot should pause (20FPS*0.05s); Manu. bl thr...: manual threshold (start with 0.95), Manu.tunr: manual slider for 0 degree of backwards turn, 0 is straight back, speed is fixed for 0.3\n",
    "\n",
    "The steering slider will display our estimated steering value.  Please remember, this value isn't the actual angle of the target, but simply a value that is\n",
    "nearly proportional.  When the actual angle is ``0``, this will be zero, and it will increase / decrease with the actual angle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "\n",
    "#category_widget.value=0\n",
    "score_widgets = []\n",
    "\n",
    "category_widget = ipywidgets.Dropdown(options=CATEGORIES_classification, description='category')\n",
    "\n",
    "#TB  sliding average for reducing noise level of angle\n",
    "#category_widget.value=0\n",
    "\n",
    "def shift_and_add(new, test_list):\n",
    "# using list slicing and \"+\" operator \n",
    "# shift last element to first \n",
    "    test_list = test_list[-1:] + test_list[:-1] \n",
    "    test_list[0]= new\n",
    "    array_ave=sum(test_list)/len(test_list)\n",
    "    \n",
    "    return array_ave, test_list \n",
    "\n",
    "\n",
    "def start_category(change):\n",
    "    global category_block\n",
    "    if not CATEGORIES_classification: \n",
    "        category_block=0\n",
    "    else:\n",
    "        category_block=CATEGORIES_classification.index(category_widget.value)\n",
    "        print(category_block)\n",
    "    return category_block\n",
    "\n",
    "category_widget.observe(start_category, names='value')\n",
    "\n",
    "#repeated for initialization\n",
    "if not CATEGORIES_classification: \n",
    "    print(\"List is empty.\")\n",
    "    category_index=0;\n",
    "else:\n",
    "    category_index=CATEGORIES_classification.index(category_widget.value)   \n",
    "    for category in CATEGORIES_classification:\n",
    "        score_widget = ipywidgets.FloatSlider(min=0.0, max=1.0, description=category, orientation='vertical')\n",
    "        score_widgets.append(score_widget) \n",
    "\n",
    "    live_execution_widget = ipywidgets.HBox(score_widgets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89d79ef3daa4a1d928357c7cd4157a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "21 FPS"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c313fbf42b4c56be9c1561365b2004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='category', options=('background1', 'redlight', 'greenlight…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16c4727181e46f986f9bbe8b613c710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='y', max=1.5, min=-1.5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40027b45a510464f897dfab0dc8eabe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='x', max=1.5, min=-1.5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af20404d9fe429f8ab4cce6e1db23e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='speed', max=2.0, min=-2.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49f58f2d68e4630a52877dd43fe681b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering', max=1.0, min=-1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=-2, max=2.0, orientation='horizontal', description='speed')\n",
    "\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.09, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.24, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "\n",
    "#display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\n",
    "robot_control=widgets.VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider])\n",
    "\n",
    "\n",
    "#behavior\n",
    "blocked_slider = ipywidgets.FloatSlider(description='blocked', min=0.0, max=1.2, orientation='horizontal')\n",
    "stopduration_slider= ipywidgets.IntSlider(min=1, max=1000, step=1, value=40, description='Manu. time stop') #anti-collision stop time\n",
    "block_threshold= ipywidgets.FloatSlider(min=0, max=1.2, step=0.01, value=0.9, description='Manu. bl threshold') #anti-collision block probability\n",
    "\n",
    "turn_degree= ipywidgets.FloatSlider(min=-90, max=90, step=0.1, value=0, description='Manu. turn degree')\n",
    "speedblock_gain_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, step=0.01, description='speed block')\n",
    "\n",
    "behavior_control=widgets.VBox([blocked_slider,block_threshold, stopduration_slider, turn_degree,speedblock_gain_slider])\n",
    "\n",
    "#category_widget = ipywidgets.Dropdown(options=np.array(CATEGORIES_classification), description='category')\n",
    "prediction_widget = ipywidgets.Text(description='prediction')\n",
    "\n",
    "\n",
    "\n",
    "predictions_disp=widgets.VBox([category_widget,prediction_widget,robot_control])\n",
    "\n",
    "\n",
    "display(widgets.HBox([target_widget]))\n",
    "d2 = IPython.display.display(\"\", display_id=2)\n",
    "display(widgets.HBox([predictions_disp,behavior_control,live_execution_widget]))\n",
    "display(y_slider, x_slider, speed_slider, steering_slider)\n",
    "\n",
    "#create new New View for Output\n",
    "#once a slider is blue then arrow keys left right can be used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. Compute the approximate steering value\n",
    "4. Control the motors using proportional / derivative control (PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "count=0\n",
    "count_stops=0\n",
    "stop_time=50 #(for how many frames the bot should go backwards, see and of script)\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "go_on=1\n",
    "max_x=camera.width\n",
    "min_x=0\n",
    "max_y=camera.height\n",
    "min_y=0\n",
    "x=0\n",
    "y=0\n",
    "test_list=[0,0,0,0,0,0]\n",
    "to_radian=math.pi/180\n",
    "category_block=0\n",
    "\n",
    "def execute(change):\n",
    "    global angle, angle_last, count, count_stops,stop_time,go_on,block_threshold,max_x, min_x,max_y,min_y,x,y, test_list,to_radian, turn_degree, category_block,category_roadindex\n",
    "    count +=1\n",
    "    t1 = time.time()\n",
    "    \n",
    "    image = change['new']\n",
    "    image_preproc = preprocess(image)\n",
    "    #anti_collision model)-----\n",
    "    output=model_trt_classification(image_preproc)\n",
    "    output = F.softmax(output, dim=1).detach().cpu().numpy().flatten()\n",
    "    category_index = output.argmax()\n",
    "    prediction_widget.value = CATEGORIES_classification[category_index]\n",
    "        \n",
    "    #chose class or add behavior------------------------\n",
    "    for i, score in enumerate(list(output)):\n",
    "        score_widgets[i].value = score\n",
    "            \n",
    "    prob_blocked = float(output.flatten()[category_block]) #second class\n",
    "    #prediction_widget.value=str(category_block)\n",
    "    blocked_slider.value = prob_blocked\n",
    "    stop_time=stopduration_slider.value\n",
    "   \n",
    "    if go_on==1:\n",
    "        if prob_blocked > block_threshold.value: #in case collision avoidance model hast recognized an object (not specific one) (prob_blocked) then stop for some time about ~ frames*0.05ms \n",
    "            count_stops +=1\n",
    "            go_on=2\n",
    "            #anti_collision------- behaviour is defined after the second else we loss one frame:\n",
    "        else: #start road following if no object\n",
    "            go_on=1\n",
    "            count_stops=0\n",
    "            #-------------------\n",
    "            xy = model_trt(image_preproc).detach().float().cpu().numpy().flatten()  \n",
    "            x = float(xy[2*category_roadindex])\n",
    "            y = float(xy[2*category_roadindex + 1] )  \n",
    "            x = int(max_x * (x / 2.0 + 0.5))\n",
    "            y = int(max_y * (y / 2.0 + 0.5))\n",
    "            x_joysticklike=((x-max_x/2.0)-min_x)/(max_x-min_x)\n",
    "            y_joysticklike=((max_y-y)-min_y)/(max_y-min_y)     \n",
    "            x_slider.value = x_joysticklike\n",
    "            y_slider.value = y_joysticklike\n",
    "            angle = np.arctan2(x_joysticklike, y_joysticklike)\n",
    "            speed_slider.value = speed_gain_slider.value #\n",
    "    else:\n",
    "        count_stops=count_stops+1\n",
    "        if count_stops<stop_time: #how many frames bot should pause, anti_collision-----------------add behavior here, backward -0.5, turn values from 0-224\n",
    "            #here could be added a behavior function (def) for the different classed------------------------ returning angle of movement in degree_choice\n",
    "            degree_choice=turn_degree.value #in degree keep between -45 (left) to 45 degree (right)  \n",
    "            angle=degree_choice*to_radian            \n",
    "            speed_slider.value=speedblock_gain_slider.value #-0.3 # speedblock_gain_slider.value set speed zero or negative bewtween -1 to 1\n",
    "            x_slider.value = (speed_slider.value*math.tan(angle))\n",
    "            y_slider.value = abs(speed_slider.value)\n",
    "        else:\n",
    "            go_on=1\n",
    "            count_stops=0\n",
    "        \n",
    "    \n",
    "     \n",
    "    #---------        \n",
    "       \n",
    "    #angle, test_list=shift_and_add(angle, test_list) #reduce noise by rolling average of 6 frames \n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle \n",
    "    steering_slider.value = pid + steering_bias_slider.value\n",
    "    if speed_slider.value>=0:\n",
    "        robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "        robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "    else:\n",
    "        robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 0), -1.0)\n",
    "        robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 0), -1.0)\n",
    "        \n",
    "    #---------    \n",
    "    \n",
    "    t2 = time.time()\n",
    "    s = f\"\"\"{int(1/(t2-t1))} FPS\"\"\"\n",
    "    d2.update(IPython.display.HTML(s) )\n",
    "    \n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \n",
    "\n",
    "You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow track.\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "camera.unobserve(execute, names='value')\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "That's it for this live demo! Hopefully you had some fun seeing your JetBot moving smoothly on track follwing the road!!!\n",
    "\n",
    "If your JetBot wasn't following road very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios and the JetBot should get even better :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
